---
title: "The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input"
source: "https://arxiv.org/html/2501.03200v1"
author: "Alon Jacovi, Andrew Wang, Chris Alberti, et al."
published: "2025-01-06"
created: 2025-06-03
description: "Research paper introducing FACTS Grounding leaderboard for benchmarking LLMs' factual accuracy in long-form responses based on context documents up to 32k tokens."
tags:
  - type/literature
  - theme/research
  - theme/learning
  - source/arxiv
  - role/author
  - target/starterkit
--- 
arXiv:2501.03200v1 \[cs.CL\] 06 Jan 2025

facts-leaderboard@google.com

Alon Jacovi Equal Contribution Andrew Wang Equal Contribution Chris Alberti Equal Contribution Connie Tao Equal Contribution Jon Lipovetz Equal Contribution Kate Olszewska Equal Contribution Lukas Haas Equal Contribution Michelle Liu Equal Contribution Nate Keating Equal Contribution Adam Bloniarz Carl Saroufim Corey Fry Dror Marcus Doron Kukliansky Gaurav Singh Tomar James Swirhun Jinwei Xing Lily Wang Madhu Gurumurthy Michael Aaron Moran Ambar Rachana Fellinger Rui Wang Zizhao Zhang Sasha Goldshtein Dipanjan Das Equal Contribution

###### Abstract

We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models’ ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at [https://www.kaggle.com/facts-leaderboard](https://www.kaggle.com/facts-leaderboard).

## 1 Introduction

Factuality is one of the most challenging aspects of Large Language Models (LLMs), referring to a model’s ability to generate factually accurate responses in information-seeking scenarios. Commonly, this area of research can be divided into two distinct scenarios: (1) factuality with respect to given context, such as a user request and grounding documents, such that the model response is fully grounded in the input (by this, we imply that a model response has the highest degree of faithfulness to given context as defined by [^24]), and (2) factuality with respect to external sources and general world knowledge  ([^29], cf. [^24]). There may be subtle use cases that fall in the middle, but most existing literature in the factuality area focus on behaviors that map to these two distinct scenarios. Summarization is a narrow but important example of the first scenario–a summary’s various claims should be accurate with respect to the source documents that are being summarized [^3]. Generating short-form, factually accurate answers to factoid questions from an LLM’s parametric knowledge is an example use case of the second scenario, as recently discussed by [^32].

Ensuring factual accuracy while generating LLM responses is challenging. The principal challenges in LLM factuality are modeling (i.e., architecture, training, and inference) and measurement (i.e., evaluation methodology, data and metrics). On the modeling front, LLMs are inherently difficult to optimize for this goal: typically, LLM pretraining is optimized to predict the next token given previous tokens in the text. While this objective may teach models salient world knowledge, it does not directly optimize the model towards the various factuality scenarios, instead encouraging the model to generate generally plausible text. Furthermore, critically, recent research suggests that the training process (regardless of specific models or training data) inherently admits non-factual text generation [^11]. Subsequent post-training, via supervised finetuning and reinforcement learning, can steer the model towards improved factuality [^15]. Other mitigation proposals include inference-time methods, such as prompting or model state interpretability [^16]. However, enhancing factuality through these methods can compromise other desirable attributes, such as creativity and novelty, which are also optimized during these stages. This creates a delicate balance, making it difficult to simultaneously achieve all desired outcomes [^25].

Due to the above, factuality is expected to remain a research challenge for the foreseeable future. In this work, we focus on the challenge of measuring progress in factuality. Measurement is in itself difficult, due to the different model behaviors that pertain to the aforementioned factuality scenarios. Some settings are more difficult than others: in long-form generation tasks, the subject of our benchmark, each claim in the models’ responses should be thoroughly inspected for their accuracy. This is in contrast to tasks that require measuring the factuality of short responses to questions against ground truth [^32]. To complicate matters further, long-form generation settings are in themselves varied and numerous. For example, some benchmarks focus on long-form generation in the setting of grounding to external sources [^33] while others focus on a particular task or domain, such as summarization to news or biomedical articles [^30].

Here, we investigate a benchmark that focuses on scenario (1)–particularly, the measurement of response factuality with respect to a provided context document of length up to 32k tokens, given varied user requests. This setting requires the model to synthesize information derivable from the document while directly addressing the request. While encompassing summarization as a key use case, it extends to a broader range of requests, including fact finding, analyzing and comparing information, and so forth, while being fully grounded to the input document. We believe that this benchmark fills a gap in evaluating a wider variety of model behaviors pertaining to factuality, in comparison to benchmarks that focus on narrower use cases, e.g. summarization alone [^30]. (Note however that we do not capture scenario (2) in this work.)

Table 1: Examples from FACTS Grounding (Public).

| System Instruction | Context Document Description | Context Tokens | User Request |
| --- | --- | --- | --- |
| Answer the question using only the information provided in the context. Do not rely on external knowledge or sources. | The development and deployment of an autonomous robotic system designed to clean skyscraper windows, highlighting its technological advancements, safety implications, and potential impact on the window-washing industry. | $\sim$ 1.1k | My sister and her dog live in NYC. I’ve visited there and have always been fascinated with their tall buildings. Then I thought…someone has to clean those! Then next thing you know, window washing robotos popped up on my feed. How do these robots work? Also what does this mean for the people who do those jobs? |
| Provide a response based solely on the information provided in the prompt. External sources and prior knowledge must not be used. | Legal interpretations and effects of the medical marijuana appropriations rider on federal marijuana prosecutions, focusing on differing circuit court approaches to determining compliance with state medical marijuana laws. | $\sim$ 1.6k | What did the first circuit conclude? |
| This task requires you to answer questions based solely on the information provided in the prompt. You are not allowed to use any external resources or prior knowledge. Present your answer in headed sections with an explanation for each section. Each explanation should be in bullet points with exactly three bullet points. | Comparison of different economic systems, including free market, command, and mixed economies, highlighting their key characteristics, advantages, and disadvantages. | $\sim$ 0.9k | which famous economists are mentioned? |
| Provide your response in a professional and formal tone. Use the information given in the document without referring to external sources or requiring additional context. Avoid using technical jargon or acronyms that are not explained within the document. | Compilation of money-saving tips for college students, categorized into recreation and entertainment, food and basic needs, clothing, budgeting/spending plan, transportation, savings, and conserving resources. | $\sim$ 1.6k | What are some tips on saving money? |
| Answer the question based solely on the information provided in the passage. Do not use any external knowledge or resources. | A study that investigates the correlation between advanced maternal age (40+) and increased risk of obstetric, fetal, and neonatal complications compared to women aged 25-35. | $\sim$ 2.1k | Researchers at Foch Hospital in France published this study of pregnancy outcomes in two groups of patients. Please summarize outcomes across the three kinds of complications that the researchers studied. |

Factuality of long-form responses is difficult to thoroughly measure at scale; particularly automatic evaluation methods continue to be a challenge and is an active research area [^19]. A particular limitation of existing automatic evaluation methods is that specialized factuality classifiers are limited to short context windows, or ones that cannot perform reasoning that is required to evaluate factuality behaviors [^10]. Here, we rigorously evaluate our automatic evaluators on held-out test data to validate their performance on our task, and use multiple aggregations to mitigate evaluator bias.

We present the FACTS Grounding leaderboard and an associated benchmark measuring the ability of LLMs to ground long-form responses to document context up to length 32k tokens given a user request and additional instructions. The benchmark contains 860 public examples (“Open” split) and 859 private examples (“Blind” split) of natural, complex LLM prompts written by humans to evaluate long-form grounded response generation (see examples in [Table 1](https://arxiv.org/html/2501.03200v1#S1.T1 "In 1 Introduction ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"); details in [Section 2](https://arxiv.org/html/2501.03200v1#S2 "2 Data ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). The leaderboard reports various LLMs’ performance on this benchmark using an automated factuality score incorporating an eligibility filter to avoid “hacking” the leaderboard metric ([Section 3](https://arxiv.org/html/2501.03200v1#S3 "3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). The results are available in [Section 4](https://arxiv.org/html/2501.03200v1#S4 "4 Results ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"). The leaderboard will be actively maintained and updated to include new models and their variants.

## 2 Data

Underlying the FACTS Grounding leaderboard is a carefully curated collection of documents and associated user requests that were written by human raters and then subjected to thorough validation and filtering. The complete methodology is outlined in the following subsections. [Table 1](https://arxiv.org/html/2501.03200v1#S1.T1 "In 1 Introduction ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input") provides concrete examples of data instances in the collection. To ensure the reliability of the benchmark, both public and private leaderboard splits were constructed using a balanced random sampling strategy.

![Refer to caption](https://arxiv.org/html/x1.png)

Figure 1: Distributions of context domain and of task requested by the user as a percent of the total set of prompts in the benchmark.

### 2.1 Annotation

In order to create our evaluation set, third-party human raters were instructed to design prompts requiring the processing of long-form input and the writing of long-form output. These tasks include Q&A, summarization, and document rewriting. Each example within our evaluation set consists of a context, which is a document or set of reviews sourced from the web, paired with a non-trivial user request that can be addressed using the provided context, necessitating a long-form response. Additionally, each example includes a system instruction directing the model to generate its response exclusively from the given context, without incorporating external knowledge.

To ensure the diversity of the evaluation set, prompts were generated across a range of document lengths (up to 32k tokens) and various enterprise domains, including finance, technology, retail, medical, and legal. The annotation instructions were carefully designed to avoid prompts requiring creative responses, expert-level domain knowledge, mathematical or logical reasoning, or meta-analysis of the text, such as tone analysis or interpretation of author intent. The specific distributions of enterprise domains and of tasks requested by users are shown in Figure [1](https://arxiv.org/html/2501.03200v1#S2.F1 "Figure 1 ‣ 2 Data ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input").

### 2.2 Data Quality Assurance

We manually verified all examples after annotation to discard those that did not align with our instructions. In particular, we ensured that all task instructions required the model to exclusively rely on the provided context, and we further removed creative-writing tasks. We also verified that the user requests were non-trivial and did not require domain expertise, mathematical knowledge, or complex reasoning. We additionally removed documents originating from PDFs where optical character recognition (OCR) rendered them unreadable. After our data quality assurance, the final dataset contained context documents with a mean length of 2.5k tokens and a maximum length of 32k tokens.

#### On data contamination.

As the context documents were collected from the internet, it is possible that they were included in models’ pre-training corpora. Although this is noteworthy, we make the following arguments towards the value of this benchmark:

1. The user requests and system instructions, which instruct specifically to only follow information in the context document, are non-contaminated. Responding to novel requests about non-novel documents is an important use-case of language models, and grounding is integral to it. This is unlike many currently available factuality benchmarks which repurpose academic tasks that have likely been contaminated [^26].
2. Our factuality score evaluates a distinct dimension of model performance that is not optimized during pre-training. Specifically, it measures the model’s ability to generate responses grounded exclusively in the provided context. This means that the model must not incorporate external knowledge, even if conflicting with the context document, and should also avoid drawing upon any pre-training knowledge to fulfill the user’s request.
3. As all frontier language models models were trained on large corpora of web data, parity is maintained for the purpose of a leaderboard.

## 3 Metrics

The final factuality score in FACTS Grounding is calculated as an aggregation of factuality verdicts from multiple judge models after the disqualification of ineligible responses that do not sufficiently succeed at responding to the user request.

Table 2: Evaluation of different judge models and evaluation prompts on a private test-set (N=402, class ratio 87:13; see § [3.1](https://arxiv.org/html/2501.03200v1#S3.SS1 "3.1 Unadjusted Factuality Score ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). Chosen prompt template for each model via Macro-F <sub>1</sub> in bold.

<table><tbody><tr><td><span>Judge Model</span></td><td><span>Prompt Template</span></td><td><span>Macro-F <sub><span>1</span></sub></span></td><td><span>Acc.</span></td><td><span>FPR</span></td><td><span>FNR</span></td><td><span>F <sub><span>1</span></sub> (<math><semantics><mo>+</mo> <annotation>+</annotation> <annotation>+</annotation></semantics></math>)</span></td><td><span>F <sub><span>1</span></sub> (<math><semantics><mo>−</mo> <annotation>-</annotation> <annotation>-</annotation></semantics></math>)</span></td></tr><tr><td rowspan="7"><span>Claude 3.5 Sonnet</span></td><td><span>Span-level</span></td><td><span>68.85</span></td><td><span>77.83</span></td><td><span>20.97</span></td><td><span>22.38</span></td><td><span>85.58</span></td><td><span>52.13</span></td></tr><tr><td><span>Implicit span-level</span></td><td><span>70.24</span></td><td><span>83.50</span></td><td><span>45.16</span></td><td><span>11.34</span></td><td><span>90.10</span></td><td><span>50.37</span></td></tr><tr><td><span>Response-level</span></td><td><span>61.88</span></td><td><span>83.25</span></td><td><span>72.58</span></td><td><span>6.69</span></td><td><span>90.42</span></td><td><span>33.33</span></td></tr><tr><td><span>JSON</span></td><td><span>56.04</span></td><td><span>64.78</span></td><td><span>33.87</span></td><td><span>35.47</span></td><td><span>75.64</span></td><td><span>36.44</span></td></tr><tr><td><span>JSON (alt)</span></td><td><span>55.37</span></td><td><span>66.75</span></td><td><span>46.77</span></td><td><span>30.81</span></td><td><span>77.91</span></td><td><span>32.84</span></td></tr><tr><td><span>JSON w. double-check</span></td><td><span>49.50</span></td><td><span>54.68</span></td><td><span>25.81</span></td><td><span>48.84</span></td><td><span>65.67</span></td><td><span>33.33</span></td></tr><tr><td><span>SimpleQA template</span></td><td><span>55.39</span></td><td><span>85.22</span></td><td><span>88.71</span></td><td><span>1.45</span></td><td><span>91.87</span></td><td><span>18.92</span></td></tr><tr><td rowspan="7"><span>Gemini 1.5 Pro</span></td><td><span>Span-level</span></td><td><span>55.84</span></td><td><span>79.31</span></td><td><span>79.03</span></td><td><span>10.17</span></td><td><span>88.03</span></td><td><span>23.64</span></td></tr><tr><td><span>Implicit span-level</span></td><td><span>56.66</span></td><td><span>85.47</span></td><td><span>87.10</span></td><td><span>1.45</span></td><td><span>91.99</span></td><td><span>21.33</span></td></tr><tr><td><span>Response-level</span></td><td><span>48.82</span></td><td><span>82.02</span></td><td><span>95.16</span></td><td><span>4.07</span></td><td><span>90.04</span></td><td><span>7.59</span></td></tr><tr><td><span>JSON</span></td><td><span>71.47</span></td><td><span>86.95</span></td><td><span>56.45</span></td><td><span>5.23</span></td><td><span>92.48</span></td><td><span>50.47</span></td></tr><tr><td><span>JSON (alt)</span></td><td><span>66.03</span></td><td><span>85.96</span></td><td><span>69.35</span></td><td><span>4.07</span></td><td><span>92.05</span></td><td><span>40.00</span></td></tr><tr><td><span>JSON w. double-check</span></td><td><span>64.89</span></td><td><span>76.35</span></td><td><span>37.10</span></td><td><span>21.22</span></td><td><span>84.95</span></td><td><span>44.83</span></td></tr><tr><td><span>SimpleQA template</span></td><td><span>51.54</span></td><td><span>84.73</span></td><td><span>93.55</span></td><td><span>1.16</span></td><td><span>91.64</span></td><td><span>11.43</span></td></tr><tr><td rowspan="7"><span>GPT-4o</span></td><td><span>Span-level</span></td><td><span>63.08</span></td><td><span>81.53</span></td><td><span>64.52</span></td><td><span>10.17</span></td><td><span>89.18</span></td><td><span>36.97</span></td></tr><tr><td><span>Implicit span-level</span></td><td><span>55.43</span></td><td><span>83.99</span></td><td><span>87.10</span></td><td><span>3.20</span></td><td><span>91.11</span></td><td><span>19.75</span></td></tr><tr><td><span>Response-level</span></td><td><span>51.54</span></td><td><span>84.73</span></td><td><span>93.55</span></td><td><span>1.16</span></td><td><span>91.64</span></td><td><span>11.43</span></td></tr><tr><td><span>JSON</span></td><td><span>69.68</span></td><td><span>80.54</span></td><td><span>32.26</span></td><td><span>17.15</span></td><td><span>87.83</span></td><td><span>51.53</span></td></tr><tr><td><span>JSON (alt)</span></td><td><span>66.78</span></td><td><span>82.02</span></td><td><span>53.23</span></td><td><span>11.63</span></td><td><span>89.28</span></td><td><span>44.27</span></td></tr><tr><td><span>JSON w. double-check</span></td><td><span>57.62</span></td><td><span>64.04</span></td><td><span>17.74</span></td><td><span>39.24</span></td><td><span>74.11</span></td><td><span>41.13</span></td></tr><tr><td><span>SimpleQA template</span></td><td><span>47.04</span></td><td><span>83.74</span></td><td><span>98.39</span></td><td><span>1.45</span></td><td><span>91.13</span></td><td><span>2.94</span></td></tr></tbody></table>

### 3.1 Unadjusted Factuality Score

The principal component of our evaluation process is an unadjusted factuality score.

First, we utilize a language model judge to produce a binary classification label identifying whether a full model response is grounded in the user request and the context document given an instruction (see Table [1](https://arxiv.org/html/2501.03200v1#S1.T1 "Table 1 ‣ 1 Introduction ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). A model response is marked with a positive label (“accurate”) if all the claims in the response are grounded in the contents of the prompt, or do not require grounding; the response is marked with a negative label (“not accurate”) if a single claim that bears information is deemed to be not grounded in the contents of the prompt. We use three different judge models in order to reduce the bias of a particular judge model, as models have been shown to be biased towards favorably judging their own outputs [^31]. The judges are prompted language models in all cases— Gemini 1.5 Pro [^7], GPT-4o [^1], and Claude 3.5 Sonnet [^2]. To select the right judge prompt, we tested seven different prompt templates and evaluated them based on alignment with human judgements on a held-out private set of prompts and model responses (N=402) which were annotated with golden labels. The prompt templates used in the evaluation are in [Appendix A](https://arxiv.org/html/2501.03200v1#A1 "Appendix A Judge Prompt Templates ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"), and results are shown in  [Table 2](https://arxiv.org/html/2501.03200v1#S3.T2 "In 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"). For each judge model, we select the best-performing prompt template via Macro-F <sub>1</sub>.

Given the three judges, the individual factuality score for each judge is the percentage of accurate responses, and the unadjusted factuality score is the average of all judge models’ scores.

### 3.2 Disqualifying Ineligible Responses

Table 3: Examples of ineligible responses: these responses, while fully grounded in the context document, fail to address the user request meaningfully and are consequently considered ineligible.

| Context Document Description | User Request | Ineligible Response | Rationale |
| --- | --- | --- | --- |
| A research report on renewable energy sources, including wind, solar, and hydroelectric power, with specific statistics and case studies. | Can you summarize the key advantages and disadvantages of wind energy from this document? | Wind energy is good because it’s renewable and clean, but it has some challenges too. | (1) The response is extremely vague, failing to provide any detailed or specific points from the document, such as the cost-effectiveness, geographic limitations, or impacts on wildlife. (2) It doesn’t engage with the query’s focus on "key" advantages and disadvantages. |
| A company’s annual financial report, discussing quarterly earnings, expenditures, future investments, and an analysis of the market environment. | Summarize the main reasons the company’s revenue decreased in Q3. | The company faced challenges in Q3 that impacted its revenue. | (1) The response avoids specifying any reasons, such as market trends, increased competition, or operational setbacks, which would likely be in the document. (2) It doesn’t demonstrate an attempt to engage with or extract relevant details. |
| A historical article on the causes and consequences of the Great Depression. | What were the main causes of the Great Depression as explained in the document? | The Great Depression was a difficult time in history with many causes and effects. | (1) The response provides no substantive information on the causes, such as stock market speculation, bank failures, or trade policies, which were discussed in the document. (2) It ignores the user’s explicit focus on the "main causes." |

Table 4: Evaluation of prompt template for the eligible responses detection task on a private test-set (N=450; see § [3.2](https://arxiv.org/html/2501.03200v1#S3.SS2 "3.2 Disqualifying Ineligible Responses ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). Chosen prompt template for each model via Macro-F <sub>1</sub> in bold.

<table><tbody><tr><td><span>Judge Model</span></td><td><span>Prompt Template</span></td><td><span>Macro-F <sub><span>1</span></sub></span></td><td><span>Acc.</span></td><td><span>FPR</span></td><td><span>FNR</span></td><td><span>F <sub><span>1</span></sub> (<math><semantics><mo>+</mo> <annotation>+</annotation> <annotation>+</annotation></semantics></math>)</span></td><td><span>F <sub><span>1</span></sub> (<math><semantics><mo>−</mo> <annotation>-</annotation> <annotation>-</annotation></semantics></math>)</span></td></tr><tr><td rowspan="2"><span>Claude 3.5 Sonnet</span></td><td><span>User request only</span></td><td><span>60.88</span></td><td><span>68.22</span></td><td><span>16.33</span></td><td><span>62.67</span></td><td><span>43.92</span></td><td><span>77.83</span></td></tr><tr><td><span>User request and context document</span></td><td><span>58.14</span></td><td><span>69.56</span></td><td><span>8.67</span></td><td><span>74.00</span></td><td><span>36.28</span></td><td><span>80.00</span></td></tr><tr><td rowspan="2"><span>Gemini 1.5 Pro</span></td><td><span>User request only</span></td><td><span>56.28</span></td><td><span>67.11</span></td><td><span>12.33</span></td><td><span>74.00</span></td><td><span>34.51</span></td><td><span>78.04</span></td></tr><tr><td><span>User request and context document</span></td><td><span>47.56</span></td><td><span>68.44</span></td><td><span>1.33</span></td><td><span>92.00</span></td><td><span>14.46</span></td><td><span>80.65</span></td></tr><tr><td rowspan="2"><span>GPT-4o</span></td><td><span>User request only</span></td><td><span>55.16</span></td><td><span>69.56</span></td><td><span>5.33</span></td><td><span>80.67</span></td><td><span>29.74</span></td><td><span>80.57</span></td></tr><tr><td><span>User request and context document</span></td><td><span>50.55</span></td><td><span>69.56</span></td><td><span>1.33</span></td><td><span>88.67</span></td><td><span>19.88</span></td><td><span>81.21</span></td></tr></tbody></table>

Metrics that are focused on evaluating the factuality of the generated text, with respect to a context document or otherwise, can be circumvented by ignoring the intent behind the user request. By giving shorter responses that evade conveying comprehensive information, even if such content was an important aspect of a user request, it is possible to achieve a high factuality score while not providing a helpful response. See illustrative examples in [Table 3](https://arxiv.org/html/2501.03200v1#S3.T3 "In 3.2 Disqualifying Ineligible Responses ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input").

We safeguard against such responses by detecting them with prompted LLM judges that use the same base models as in [Section 3.1](https://arxiv.org/html/2501.03200v1#S3.SS1 "3.1 Unadjusted Factuality Score ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"), with prompt templates described in [Appendix A](https://arxiv.org/html/2501.03200v1#A1 "Appendix A Judge Prompt Templates ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input"). At a response level, similar to unadjusted factuality score, we treat instruction-following as a distinct task for an LLM judge. This task involves a binary classification of each model response, determining whether it sufficiently addresses the user’s request. Each response is assigned a binary label indicating its eligibility: either “eligible,” signifying that it answers the user’s request, or “ineligible,” otherwise. Ineligible responses are disqualified from factuality evaluation and the final factuality score is adjusted such that ineligible responses are deemed as inaccurate.

We investigate prompting each LLM judge with a prompt that contains either the user request only or the user request and the context document. We evaluate these two prompt templates on a separate test set which includes golden labels for instruction-following eligibility and assess whether the predicted ratings align with these golden labels (see Table [4](https://arxiv.org/html/2501.03200v1#S3.T4 "Table 4 ‣ 3.2 Disqualifying Ineligible Responses ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input") for results). For each LLM judge, we select the prompt with the highest Macro-F <sub>1</sub>. Per response, eligibility classifications across the three judges are ensembled by consensus. A response is considered ineligible only if all three judges consider the response ineligible. Ensembling via consensus focuses this benchmark on evaluating grounding while still filtering out the worst quality responses.

Table 5: FACTS Grounding results representing unadjusted factuality score (no disqualification). Results are reported with a 95% confidence interval.

<table><tbody><tr><td rowspan="4"><span>Response Model</span></td><td colspan="6"><span>Judge Models</span></td><td rowspan="4"><span>Average</span></td><td><span>Fused</span></td></tr><tr><td colspan="3"><span>Open (N=860)</span></td><td colspan="3"><span>Blind (N=859)</span></td><td rowspan="3"><span>Rank</span></td></tr><tr><td><span>Gemini 1.5</span></td><td><span>GPT-4o</span></td><td><span>Claude 3.5</span></td><td><span>Gemini 1.5</span></td><td><span>GPT-4o</span></td><td><span>Claude 3.5</span></td></tr><tr><td><span>Pro</span></td><td></td><td><span>Sonnet</span></td><td><span>Pro</span></td><td></td><td><span>Sonnet</span></td></tr><tr><td><span>Gemini 1.5 Flash</span></td><td><span>91.4</span> <sub><span>(±1.9)</span></sub></td><td><span>82.0</span> <sub><span>(±2.6)</span></sub></td><td><span>84.9</span> <sub><span>(±2.4)</span></sub></td><td><span>90.7</span> <sub><span>(±1.9)</span></sub></td><td><span>80.7</span> <sub><span>(±2.6)</span></sub></td><td><span>85.1</span> <sub><span>(±2.4)</span></sub></td><td><span>85.8 <sub><span>(±1.7)</span></sub></span></td><td><span>1</span></td></tr><tr><td><span>Gemini 2.0 Flash Experimental</span></td><td><span>88.7</span> <sub><span>(±2.1)</span></sub></td><td><span>79.5</span> <sub><span>(±2.7)</span></sub></td><td><span>86.7</span> <sub><span>(±2.3)</span></sub></td><td><span>91.5</span> <sub><span>(±1.9)</span></sub></td><td><span>79.3</span> <sub><span>(±2.7)</span></sub></td><td><span>88.0</span> <sub><span>(±2.2)</span></sub></td><td><span>85.6</span> <sub><span>(±1.7)</span></sub></td><td><span>2</span></td></tr><tr><td><span>Gemini 1.5 Pro</span></td><td><span>90.2</span> <sub><span>(±2.0)</span></sub></td><td><span>76.0</span> <sub><span>(±2.9)</span></sub></td><td><span>83.6</span> <sub><span>(±2.5)</span></sub></td><td><span>89.8</span> <sub><span>(±2.0)</span></sub></td><td><span>73.2</span> <sub><span>(±3.0)</span></sub></td><td><span>83.5</span> <sub><span>(±2.5)</span></sub></td><td><span>82.7</span> <sub><span>(±1.8)</span></sub></td><td><span>3</span></td></tr><tr><td><span>Claude 3.5 Sonnet</span></td><td><span>88.4</span> <sub><span>(±2.1)</span></sub></td><td><span>72.7</span> <sub><span>(±3.0)</span></sub></td><td><span>87.7</span> <sub><span>(±2.2)</span></sub></td><td><span>88.4</span> <sub><span>(±2.1)</span></sub></td><td><span>73.8</span> <sub><span>(±2.9)</span></sub></td><td><span>82.2</span> <sub><span>(±2.6)</span></sub></td><td><span>82.2</span> <sub><span>(±1.8)</span></sub></td><td><span>4</span></td></tr><tr><td><span>GPT-4o</span></td><td><span>86.7</span> <sub><span>(±2.3)</span></sub></td><td><span>71.7</span> <sub><span>(±3.0)</span></sub></td><td><span>79.2</span> <sub><span>(±2.7)</span></sub></td><td><span>88.0</span> <sub><span>(±2.2)</span></sub></td><td><span>75.9</span> <sub><span>(±2.9)</span></sub></td><td><span>77.4</span> <sub><span>(±2.8)</span></sub></td><td><span>79.8</span> <sub><span>(±1.9)</span></sub></td><td><span>5</span></td></tr><tr><td><span>Claude 3.5 Haiku</span></td><td><span>85.8</span> <sub><span>(±2.3)</span></sub></td><td><span>67.2</span> <sub><span>(±3.1)</span></sub></td><td><span>82.0</span> <sub><span>(±2.6)</span></sub></td><td><span>80.1</span> <sub><span>(±2.7)</span></sub></td><td><span>62.5</span> <sub><span>(±3.2)</span></sub></td><td><span>74.3</span> <sub><span>(±2.9)</span></sub></td><td><span>75.3</span> <sub><span>(±2.0)</span></sub></td><td><span>6</span></td></tr><tr><td><span>GPT-4o mini</span></td><td><span>80.8</span> <sub><span>(±2.6)</span></sub></td><td><span>62.1</span> <sub><span>(±3.2)</span></sub></td><td><span>71.4</span> <sub><span>(±3.0)</span></sub></td><td><span>83.0</span> <sub><span>(±2.5)</span></sub></td><td><span>65.0</span> <sub><span>(±3.2)</span></sub></td><td><span>70.7</span> <sub><span>(±3.0)</span></sub></td><td><span>72.2</span> <sub><span>(±2.1)</span></sub></td><td><span>7</span></td></tr><tr><td><span>OpenAI o1-mini</span></td><td><span>70.8</span> <sub><span>(±3.0)</span></sub></td><td><span>50.2</span> <sub><span>(±3.3)</span></sub></td><td><span>63.1</span> <sub><span>(±3.2)</span></sub></td><td><span>75.1</span> <sub><span>(±2.9)</span></sub></td><td><span>51.3</span> <sub><span>(±3.3)</span></sub></td><td><span>64.6</span> <sub><span>(±3.2)</span></sub></td><td><span>62.5</span> <sub><span>(±2.3)</span></sub></td><td><span>8</span></td></tr><tr><td><span>OpenAI o1-preview</span></td><td><span>69.2</span> <sub><span>(±3.1)</span></sub></td><td><span>50.1</span> <sub><span>(±3.3)</span></sub></td><td><span>65.3</span> <sub><span>(±3.2)</span></sub></td><td><span>70.0</span> <sub><span>(±3.1)</span></sub></td><td><span>53.2</span> <sub><span>(±3.3)</span></sub></td><td><span>65.0</span> <sub><span>(±3.2)</span></sub></td><td><span>62.1</span> <sub><span>(±2.3)</span></sub></td><td><span>9</span></td></tr></tbody></table>

## 4 Results

[Table 5](https://arxiv.org/html/2501.03200v1#S3.T5 "In 3.2 Disqualifying Ineligible Responses ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input") and [Table 6](https://arxiv.org/html/2501.03200v1#S4.T6 "In On ineligible responses. ‣ 4 Results ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input") contain the unadjusted and final factuality scores before and after disqualifying ineligible responses, respectively. The tested models are Gemini 1.5 Pro and Flash [^7], Gemini 2.0 Flash Experimental [^6],GPT-4o [^1],OpenAI o1-preview and o1-mini [^20],Claude 3.5 Haiku and Sonnet [^2]. For the "Fused Rank" metric, we employ a ranking aggregation method that combines the six individual model rankings—derived from each split and judge model—into a single, unified ranking using the Condorcet algorithm. The resulting fused rank exactly aligns with the ranking obtained using the final factuality score.

#### On aggregating multiple judge models.

Consistent with prior research [^18], we found that models generally rate their own outputs higher than those of other models, exhibiting a mean increase of +3.23%. While the use of multiple judge models increases the computational cost of evaluation, this approach is essential when the judge models themselves are also subject to evaluation, as is the case in our work.

#### On ineligible responses.

Disqualifying ineligible responses leads to a reduction of $1\%$ – $5\%$ in the final factuality score, as these responses are treated as inaccurate. The disqualification process also induces a minor shift in model rankings; for instance, Gemini 1.5 Flash moves from rank 1 to rank 2.

Table 6: FACTS Grounding Results representing the final factuality score after disqualifying ineligible responses (see § [3.2](https://arxiv.org/html/2501.03200v1#S3.SS2 "3.2 Disqualifying Ineligible Responses ‣ 3 Metrics ‣ The FACTS Grounding Leaderboard: Benchmarking LLMs’ Ability to Ground Responses to Long-Form Input")). Results are reported with a 95% confidence interval.

<table><tbody><tr><td rowspan="4"><span>Response Model</span></td><td colspan="6"><span>Judge Models</span></td><td rowspan="4"><span>Average</span></td><td><span>Fused</span></td></tr><tr><td colspan="3"><span>Open (N=860)</span></td><td colspan="3"><span>Blind (N=859)</span></td><td rowspan="3"><span>Rank</span></td></tr><tr><td><span>Gemini 1.5</span></td><td><span>GPT-4o</span></td><td><span>Claude 3.5</span></td><td><span>Gemini 1.5</span></td><td><span>GPT-4o</span></td><td><span>Claude 3.5</span></td></tr><tr><td><span>Pro</span></td><td></td><td><span>Sonnet</span></td><td><span>Pro</span></td><td></td><td><span>Sonnet</span></td></tr><tr><td><span>Gemini 2.0 Flash Experimental</span></td><td><span>86.4</span> <sub><span>(±2.3)</span></sub></td><td><span>77.4</span> <sub><span>(±2.8)</span></sub></td><td><span>84.8</span> <sub><span>(±2.4)</span></sub></td><td><span>89.3</span> <sub><span>(±2.1)</span></sub></td><td><span>77.2</span> <sub><span>(±2.8)</span></sub></td><td><span>86.3</span> <sub><span>(±2.3)</span></sub></td><td><span>83.6 <sub><span>(±1.8)</span></sub></span></td><td><span>1</span></td></tr><tr><td><span>Gemini 1.5 Flash</span></td><td><span>88.1</span> <sub><span>(±2.2)</span></sub></td><td><span>79.2</span> <sub><span>(±2.7)</span></sub></td><td><span>82.6</span> <sub><span>(±2.5)</span></sub></td><td><span>87.3</span> <sub><span>(±2.2)</span></sub></td><td><span>77.9</span> <sub><span>(±2.8)</span></sub></td><td><span>82.3</span> <sub><span>(±2.6)</span></sub></td><td><span>82.9</span> <sub><span>(±1.8)</span></sub></td><td><span>2</span></td></tr><tr><td><span>Gemini 1.5 Pro</span></td><td><span>87.4</span> <sub><span>(±2.2)</span></sub></td><td><span>73.8</span> <sub><span>(±2.9)</span></sub></td><td><span>81.4</span> <sub><span>(±2.6)</span></sub></td><td><span>86.5</span> <sub><span>(±2.3)</span></sub></td><td><span>70.2</span> <sub><span>(±3.1)</span></sub></td><td><span>80.8</span> <sub><span>(±2.6)</span></sub></td><td><span>80.0</span> <sub><span>(±1.9)</span></sub></td><td><span>3</span></td></tr><tr><td><span>Claude 3.5 Sonnet</span></td><td><span>87.3</span> <sub><span>(±2.2)</span></sub></td><td><span>71.7</span> <sub><span>(±3.0)</span></sub></td><td><span>86.7</span> <sub><span>(±2.3)</span></sub></td><td><span>82.0</span> <sub><span>(±2.6)</span></sub></td><td><span>67.8</span> <sub><span>(±3.1)</span></sub></td><td><span>81.0</span> <sub><span>(±2.6)</span></sub></td><td><span>79.4</span> <sub><span>(±1.9)</span></sub></td><td><span>4</span></td></tr><tr><td><span>GPT-4o</span></td><td><span>85.3</span> <sub><span>(±2.4)</span></sub></td><td><span>70.7</span> <sub><span>(±3.0)</span></sub></td><td><span>78.5</span> <sub><span>(±2.7)</span></sub></td><td><span>86.5</span> <sub><span>(±2.3)</span></sub></td><td><span>74.9</span> <sub><span>(±2.9)</span></sub></td><td><span>76.8</span> <sub><span>(±2.8)</span></sub></td><td><span>78.8</span> <sub><span>(±1.9)</span></sub></td><td><span>5</span></td></tr><tr><td><span>Claude 3.5 Haiku</span></td><td><span>84.8</span> <sub><span>(±2.4)</span></sub></td><td><span>66.6</span> <sub><span>(±3.2)</span></sub></td><td><span>81.2</span> <sub><span>(±2.6)</span></sub></td><td><span>77.9</span> <sub><span>(±2.8)</span></sub></td><td><span>61.0</span> <sub><span>(±3.3)</span></sub></td><td><span>73.7</span> <sub><span>(±2.9)</span></sub></td><td><span>74.2</span> <sub><span>(±2.1)</span></sub></td><td><span>6</span></td></tr><tr><td><span>GPT-4o mini</span></td><td><span>79.4</span> <sub><span>(±2.7)</span></sub></td><td><span>60.8</span> <sub><span>(±3.3)</span></sub></td><td><span>70.7</span> <sub><span>(±3.0)</span></sub></td><td><span>81.4</span> <sub><span>(±2.6)</span></sub></td><td><span>63.7</span> <sub><span>(±3.2)</span></sub></td><td><span>70.1</span> <sub><span>(±3.1)</span></sub></td><td><span>71.0</span> <sub><span>(±2.1)</span></sub></td><td><span>7</span></td></tr><tr><td><span>OpenAI o1-mini</span></td><td><span>70.2</span> <sub><span>(±3.1)</span></sub></td><td><span>49.8</span> <sub><span>(±3.3)</span></sub></td><td><span>62.7</span> <sub><span>(±3.2)</span></sub></td><td><span>74.2</span> <sub><span>(±2.9)</span></sub></td><td><span>50.8</span> <sub><span>(±3.3)</span></sub></td><td><span>64.3</span> <sub><span>(±3.2)</span></sub></td><td><span>62.0</span> <sub><span>(±2.3)</span></sub></td><td><span>8</span></td></tr><tr><td><span>OpenAI o1-preview</span></td><td><span>68.6</span> <sub><span>(±3.1)</span></sub></td><td><span>49.7</span> <sub><span>(±3.3)</span></sub></td><td><span>65.0</span> <sub><span>(±3.2)</span></sub></td><td><span>69.4</span> <sub><span>(±3.1)</span></sub></td><td><span>52.6</span> <sub><span>(±3.3)</span></sub></td><td><span>64.6</span> <sub><span>(±3.2)</span></sub></td><td><span>61.7</span> <sub><span>(±2.3)</span></sub></td><td><span>9</span></td></tr></tbody></table>

## 5 Conclusion

The FACTS Grounding leaderboard is designed to rigorously challenge language models’ ability to maintain factual accuracy when generating long-form responses grounded in a document provided within the prompt, and in accordance with a user’s specific request and instructions. We encourage other researchers to utilize this benchmark to advance both the factual capabilities of models and the methodologies for evaluating factuality.

## 6 Contributions and Acknowledgements

- Experimental design: Alon Jacovi, Andrew Wang, Chris Alberti, Jon Lipovetz, and Michelle Liu created the experimental design behind the benchmark and ran all the reported experiments.
- Organization: Connie Tao, Dipanjan Das, Kate Olszewska, Lukas Haas, and Nate Keating managed the overall organization of the effort from start to completion.
- Early experimentation: Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang and Sasha Goldshtein contributed to ideas, conducted data collection and ran early experiments.
- Slav Petrov and Madhu Gurumurthy proposed the idea for this leaderboard.

All authors wrote parts of the report.

We would also like to thank:

- Gemini team for the support and model access.
- Kaggle team for their expertise and releasing the leaderboard.
- Expert data annotators who helped to collect examples in the paper.
- Our reviewers Kellie Webster and Phoebe Kirk for valuable feedback.

## References

[^1]: J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al.GPT-4 technical report.*arXiv preprint arXiv:2303.08774*, 2023.

[^2]: Anthropic.The Claude 3 model family: Opus, Sonnet, Haiku, 2024.URL [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf).

[^3]: J. A. Bishop, Q. Xie, and S. Ananiadou.LongDocFACTScore: Evaluating the factuality of long document abstractive summarisation.*arXiv preprint arXiv:2309.12455*, 2023.

[^4]: Y. Chang, K. Lo, T. Goyal, and M. Iyyer.Booookscore: A systematic exploration of book-length summarization in the era of LLMs.*arXiv preprint arXiv:2310.00785*, 2023.

[^5]: Z. Gekhman, J. Herzig, R. Aharoni, C. Elkind, and I. Szpektor.Trueteacher: Learning factual consistency evaluation with large language models.*arXiv preprint arXiv:2305.11171*, 2023.

[^6]: Gemini Team.Introducing gemini 2.0: our new ai model for the agentic era, 2024.URL [https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024).Accessed: 2024-12-11.

[^7]: Gemini Team: R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al.Gemini: a family of highly capable multimodal models.*arXiv preprint arXiv:2312.11805*, 2023.

[^8]: O. Honovich, R. Aharoni, J. Herzig, H. Taitelbaum, D. Kukliansy, V. Cohen, T. Scialom, I. Szpektor, A. Hassidim, and Y. Matias.TRUE: Re-evaluating factual consistency evaluation.*arXiv preprint arXiv:2204.04991*, 2022.

[^9]: C.-W. Huang and Y.-N. Chen.FactAlign: Long-form factuality alignment of large language models.*arXiv preprint arXiv:2410.01691*, 2024.

[^10]: A. Jacovi, M. Ambar, E. Ben-David, U. Shaham, A. Feder, M. Geva, D. Marcus, and A. Caciularu.Coverbench: A challenging benchmark for complex claim verification.*arXiv preprint arXiv:2408.03325*, 2024.

[^11]: A. T. Kalai and S. S. Vempala.Calibrated language models must hallucinate.In *Proceedings of the 56th Annual ACM Symposium on Theory of Computing*, pages 160–171, 2024.

[^12]: M. Karpinska, K. Thai, K. Lo, T. Goyal, and M. Iyyer.One thousand and one pairs: A "novel" challenge for long-context language models.*arXiv preprint arXiv:2406.16264*, 2024.

[^13]: Y. Kim, Y. Chang, M. Karpinska, A. Garimella, V. Manjunatha, K. Lo, T. Goyal, and M. Iyyer.FABLES: Evaluating faithfulness and content selection in book-length summarization.*arXiv preprint arXiv:2404.01261*, 2024.

[^14]: K. Krishna, E. Bransom, B. Kuehl, M. Iyyer, P. Dasigi, A. Cohan, and K. Lo.LongEval: Guidelines for human evaluation of faithfulness in long-form summarization.*arXiv preprint arXiv:2301.13298*, 2023.

[^15]: Z. Lan, W. Li, J. Su, X. Xiao, J. Liu, W. Wu, and Y. Lyu.Factgen: Faithful text generation by factuality-aware pre-training and contrastive ranking fine-tuning.*Journal of Artificial Intelligence Research*, 76:1281–1303, 2023.

[^16]: S. Lee, H. Hsu, and C.-F. Chen.LLM hallucination reasoning with zero-shot knowledge test.*arXiv preprint arXiv:2411.09689*, 2024.

[^17]: J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen.HaluEval: A large-scale hallucination evaluation benchmark for large language models.*arXiv preprint arXiv:2305.11747*, 2023.

[^18]: Y. Liu, N. Moosavi, and C. Lin.LLMs as narcissistic evaluators: When ego inflates evaluation scores.In L.-W. Ku, A. Martins, and V. Srikumar, editors, *Findings of the Association for Computational Linguistics: ACL 2024*, pages 12688–12701, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics.[10.18653/v1/2024.findings-acl.753](https://arxiv.org/doi.org/10.18653/v1/2024.findings-acl.753).URL [https://aclanthology.org/2024.findings-acl.753](https://aclanthology.org/2024.findings-acl.753).

[^19]: S. Min, K. Krishna, X. Lyu, M. Lewis, W.-t. Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi.Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.*arXiv preprint arXiv:2305.14251*, 2023.

[^20]: OpenAI.Learning to reason with LLMs, 2024.URL [https://openai.com/index/learning-to-reason-with-llms](https://openai.com/index/learning-to-reason-with-llms).

[^21]: L. Pan, X. Wu, X. Lu, A. T. Luu, W. Y. Wang, M.-Y. Kan, and P. Nakov.Fact-checking complex claims with program-guided reasoning.*arXiv preprint arXiv:2305.12744*, 2023.

[^22]: S. Ramprasad and B. C. Wallace.Do automatic factuality metrics measure factuality? A critical evaluation.*arXiv preprint arXiv:2411.16638*, 2024.

[^23]: S. Ramprasad, K. Krishna, Z. C. Lipton, and B. C. Wallace.Evaluating the factuality of zero-shot summarizers across varied domains.*arXiv preprint arXiv:2402.03509*, 2024.

[^24]: H. Rashkin, V. Nikolaev, M. Lamm, L. Aroyo, M. Collins, D. Das, S. Petrov, G. S. Tomar, I. Turc, and D. Reitter.Measuring attribution in natural language generation models.*Computational Linguistics*, 49(4):777–840, 2023.

[^25]: P. Roit, J. Ferret, L. Shani, R. Aharoni, G. Cideron, R. Dadashi, M. Geist, S. Girgin, L. Hussenot, O. Keller, et al.Factually consistent summarization via reinforcement learning with textual entailment feedback.*arXiv preprint arXiv:2306.00186*, 2023.

[^26]: O. Sainz, I. García-Ferrero, A. Jacovi, J. A. Campos, Y. Elazar, E. Agirre, Y. Goldberg, W.-L. Chen, J. Chim, L. Choshen, et al.Data contamination report from the 2024 CONDA shared task.*arXiv preprint arXiv:2407.21530*, 2024.

[^27]: Y. Song, Y. Kim, and M. Iyyer.VERISCORE: Evaluating the factuality of verifiable claims in long-form text generation.*arXiv preprint arXiv:2406.19276*, 2024.

[^28]: W. Su, C. Wang, Q. Ai, Y. Hu, Z. Wu, Y. Zhou, and Y. Liu.Unsupervised real-time hallucination detection based on the internal states of large language models.*arXiv preprint arXiv:2403.06448*, 2024.

[^29]: L. Tang, P. Laban, and G. Durrett.MiniCheck: Efficient fact-checking of LLMs on grounding documents.*arXiv preprint arXiv:2404.10774*, 2024.

[^30]: Vectara.Hallucination evaluation model (revision 7437011), 2024.URL [https://huggingface.co/vectara/hallucination\_evaluation\_model](https://huggingface.co/vectara/hallucination_evaluation_model).

[^31]: K. Wataoka, T. Takahashi, and R. Ri.Self-preference bias in LLM-as-a-judge.*arXiv preprint arXiv:2410.21819*, 2024.

[^32]: J. Wei, N. Karina, H. W. Chung, Y. J. Jiao, S. Papay, A. Glaese, J. Schulman, and W. Fedus.Measuring short-form factuality in large language models.*arXiv preprint arXiv:2411.04368*, 2024a.

[^33]: J. Wei, C. Yang, X. Song, Y. Lu, N. Hu, D. Tran, D. Peng, R. Liu, D. Huang, C. Du, et al.Long-form factuality in large language models.*arXiv preprint arXiv:2403.18802*, 2024b.

[^34]: W. Xu, G. Zhu, X. Zhao, L. Pan, L. Li, and W. Wang.Pride and prejudice: LLM amplifies self-bias in self-refinement.In L.-W. Ku, A. Martins, and V. Srikumar, editors, *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 15474–15492, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics.[10.18653/v1/2024.acl-long.826](https://arxiv.org/doi.org/10.18653/v1/2024.acl-long.826).URL [https://aclanthology.org/2024.acl-long.826](https://aclanthology.org/2024.acl-long.826).

[^35]: J. Ye, Y. Wang, Y. Huang, D. Chen, Q. Zhang, N. Moniz, T. Gao, W. Geyer, C. Huang, P.-Y. Chen, N. V. Chawla, and X. Zhang.Justice or prejudice? quantifying biases in llm-as-a-judge.*arXiv preprint arXiv:2410.02736*, 2024.

[^36]: W. Zhao, T. Goyal, Y. Y. Chiu, L. Jiang, B. Newman, A. Ravichander, K. Chandu, R. L. Bras, C. Cardie, Y. Deng, et al.WildHallucinations: Evaluating long-form factuality in LLMs with real-world entity queries.*arXiv preprint arXiv:2407.17468*, 2024a.

[^37]: Y. Zhao, J. Zhang, I. Chern, S. Gao, P. Liu, J. He, et al.FELM: Benchmarking factuality evaluation of large language models.*Advances in Neural Information Processing Systems*, 36, 2024b.

[^38]: L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica.Judging llm-as-a-judge with mt-bench and chatbot arena.In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, *Advances in Neural Information Processing Systems*, volume 36, pages 46595–46623. Curran Associates, Inc., 2023.URL [https://proceedings.neurips.cc/paper\_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets\_and\_Benchmarks.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf).

[^39]: Z. Zhu, Y. Yang, and Z. Sun.HaluEval-Wild: Evaluating hallucinations of language models in the wild.*arXiv preprint arXiv:2403.04307*, 2024.